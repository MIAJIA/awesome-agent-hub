{
  "category": "education",
  "status": "alpha",
  "purpose": "Provide tutorials and examples for effectively using Triton Inference Server for deploying AI models.",
  "principle": "Demonstrates the use of Triton Inference Server to serve machine learning models with various deployment strategies and optimization techniques.",
  "reusability": "Includes reusable code examples and scripts for deploying models with Triton Inference Server.",
  "limitations": "Primarily focused on Triton Inference Server; may not cover other inference servers or deployment methods.",
  "platforms": [
    "Linux",
    "Docker"
  ],
  "stack": [
    "Python",
    "Triton Inference Server",
    "Docker"
  ],
  "name": "tutorials",
  "slug": "triton-inference-server-tutorials",
  "description": "This repository contains tutorials and examples for Triton Inference Server",
  "repository": "https://github.com/triton-inference-server/tutorials",
  "stars": 712,
  "originator": "triton-inference-server",
  "tags": [
    "Triton",
    "AI Model Deployment",
    "Inference"
  ],
  "open_source": true,
  "license": "BSD-3-Clause",
  "last_updated": "2025-05-27",
  "language": "Python",
  "useful_links": [
    "https://github.com/triton-inference-server/tutorials/blob/main/README.md",
    "https://docs.nvidia.com/deeplearning/triton-inference-server/user-guide/docs/index.html"
  ]
}