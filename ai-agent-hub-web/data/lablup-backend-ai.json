{
  "category": "infra-tools",
  "status": "alpha",
  "purpose": "To provide a container-based computing cluster platform for hosting popular computing and machine learning frameworks with diverse programming language support",
  "principle": "Utilizes containerization to manage computing resources and supports various hardware accelerators for optimized performance",
  "reusability": "Offers APIs for integration, supports multiple programming languages and frameworks, and can be customized for different hardware setups",
  "limitations": "Primarily focused on environments with heterogeneous hardware accelerators, may require specific hardware configurations",
  "platforms": [
    "Linux",
    "Docker"
  ],
  "stack": [
    "Python",
    "Docker",
    "CUDA",
    "ROCm",
    "TPU",
    "IPU"
  ],
  "name": "backend.ai",
  "slug": "lablup-backend-ai",
  "description": "Backend.AI is a streamlined, container-based computing cluster platform that hosts popular computing/ML frameworks and diverse programming languages, with pluggable heterogeneous accelerator support including CUDA GPU, ROCm GPU, TPU, IPU and other NPUs.",
  "repository": "https://github.com/lablup/backend.ai",
  "stars": 563,
  "originator": "lablup",
  "tags": [
    "api",
    "backendai",
    "cloud-computing",
    "containers",
    "distributed-computing",
    "docker",
    "documentation",
    "hpc",
    "monitoring",
    "paas",
    "python"
  ],
  "open_source": true,
  "license": "LGPL-3.0",
  "last_updated": "2025-05-29",
  "language": "Python",
  "useful_links": [
    "https://github.com/lablup/backend.ai",
    "https://backend.ai/docs",
    "https://backend.ai/tutorials"
  ],
  "highlight": "Supports pluggable heterogeneous accelerators including CUDA GPU, ROCm GPU, TPU, IPU, and other NPUs"
}